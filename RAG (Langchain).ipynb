{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Extraction\n",
    "The next step is extracting content from these files and converting them into a structured text format for further processing.\n",
    "\n",
    "#### Extracting Text from PDFs\n",
    "In working with PDFs (e.g., resumes, cover letters), we will need a library that can extract the text content. One common choice is PyMuPDF (also known as fitz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    # Open the provided PDF file\n",
    "    document = fitz.open(pdf_path)\n",
    "    text = ''\n",
    "    \n",
    "    # Iterate over each page in the PDF\n",
    "    for page in document:\n",
    "        # Extract text from the page and add it to the overall text\n",
    "        text += page.get_text()\n",
    "\n",
    "    document.close()\n",
    "    return text\n",
    "\n",
    "# Specify the path to your PDF file\n",
    "pdf_path = 'resume.pdf'\n",
    "extracted_text = extract_text_from_pdf(pdf_path)\n",
    "#print(extracted_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Text from Word Documents\n",
    "For .docx files, you can use the python-docx library to extract text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    doc = Document(docx_path)\n",
    "    text = \"\"\n",
    "    for para in doc.paragraphs:\n",
    "        text += para.text + \"\\n\"\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "#docx_text = extract_text_from_docx(\"document.docx\")\n",
    "#print(docx_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Plain Text Files\n",
    "For .txt files, you can use standard Python file handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_txt(txt_path):\n",
    "    with open(txt_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "#txt_text = extract_text_from_txt(\"notes.txt\")\n",
    "#print(txt_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Web Data\n",
    "If we want to include content from personal websites, we can use web scraping tools like BeautifulSoup for static pages or Selenium for dynamic ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_text_from_website(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    return soup.get_text()\n",
    "\n",
    "# Example usage\n",
    "web_text = extract_text_from_website(\"https://medium.com/@spaw.co/best-websites-to-practice-web-scraping-9df5d4df4d1\")\n",
    "#print(web_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the Ingested Data\n",
    "#### Text Cleaning:\n",
    "The raw text you've extracted contains some noise, such as inconsistent line breaks, special characters, and repeated segments. The first step is to clean the text:\n",
    "\n",
    "- Remove unnecessary characters: Get rid of symbols or extra spaces.\n",
    "- Handle line breaks: Convert unnecessary line breaks into spaces where appropriate.\n",
    "- Fix punctuation issues: Ensure sentence integrity (e.g., merge split sentences across lines)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and multiple spaces\n",
    "    text = re.sub(r'\\W+', ' ', text)  # Replace non-word characters with a space\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# Use the previously extracted text\n",
    "preprocessed_text = preprocess_text(extracted_text)\n",
    "#print(preprocessed_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing Text Chunks with Sentence Transformers\n",
    "This code utilizes the SentenceTransformer model to generate vector embeddings for chunks of text, which can be used for efficient retrieval or further AI processing.\n",
    "\n",
    "- Imports the SentenceTransformer model and defines the vectorize_text function to embed text chunks using the pre-trained all-MiniLM-L6-v2 model.\n",
    "- Splits the pre-processed text into smaller chunks (200 characters each).\n",
    "- Passes the text chunks to the vectorize_text function, which encodes each chunk into vector embeddings (embeddings_1), facilitating downstream tasks like retrieval or RAG-based generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda\\envs\\new_env\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def vectorize_text(chunks):\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    embeddings = model.encode(chunks)\n",
    "    return embeddings\n",
    "\n",
    "# Assuming text is chunked already\n",
    "text_chunks = [preprocessed_text[i:i+200] for i in range(0, len(preprocessed_text), 200)]\n",
    "embeddings_1 = vectorize_text(text_chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing Personal Data into ChromaDB\n",
    "This code snippet creates a collection in ChromaDB and indexes personal data by adding text chunks and their corresponding embeddings into the collection for efficient retrieval.\n",
    "\n",
    "- Initializes the ChromaDB client and creates a collection named \"personal_data\".\n",
    "- Defines the index_text function to iterate through text chunks and embeddings, adding them to the collection.\n",
    "- Each text chunk is stored with its corresponding embedding, metadata (such as id and source), and a unique ID.\n",
    "- Calls the index_text function to index the pre-processed text chunks (text_chunks) and their embeddings (embeddings_1) into the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 1\n",
      "Add of existing embedding ID: 1\n",
      "Insert of existing embedding ID: 2\n",
      "Add of existing embedding ID: 2\n",
      "Insert of existing embedding ID: 3\n",
      "Add of existing embedding ID: 3\n",
      "Insert of existing embedding ID: 4\n",
      "Add of existing embedding ID: 4\n",
      "Insert of existing embedding ID: 5\n",
      "Add of existing embedding ID: 5\n",
      "Insert of existing embedding ID: 6\n",
      "Add of existing embedding ID: 6\n",
      "Insert of existing embedding ID: 7\n",
      "Add of existing embedding ID: 7\n",
      "Insert of existing embedding ID: 8\n",
      "Add of existing embedding ID: 8\n",
      "Insert of existing embedding ID: 9\n",
      "Add of existing embedding ID: 9\n",
      "Insert of existing embedding ID: 10\n",
      "Add of existing embedding ID: 10\n",
      "Insert of existing embedding ID: 11\n",
      "Add of existing embedding ID: 11\n",
      "Insert of existing embedding ID: 12\n",
      "Add of existing embedding ID: 12\n",
      "Insert of existing embedding ID: 13\n",
      "Add of existing embedding ID: 13\n",
      "Insert of existing embedding ID: 14\n",
      "Add of existing embedding ID: 14\n",
      "Insert of existing embedding ID: 15\n",
      "Add of existing embedding ID: 15\n",
      "Insert of existing embedding ID: 16\n",
      "Add of existing embedding ID: 16\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "\n",
    "client = chromadb.Client()\n",
    "collection = client.create_collection(\"personal_data\", get_or_create=True)\n",
    "\n",
    "def index_text(collection, text_chunks, embeddings):\n",
    "    for i, chunk in enumerate(text_chunks):\n",
    "        collection.add(\n",
    "            documents=[chunk],\n",
    "            embeddings=[embeddings[i]],\n",
    "            metadatas=[{\"id\": str(i), \"source\": \"resume\"}],\n",
    "            ids=[str(i)]\n",
    "        )\n",
    "\n",
    "index_text(collection, text_chunks, embeddings_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Embedding and Retrieval from ChromaDB\n",
    "\n",
    "- Defines the model for embedding text using SentenceTransformer('all-MiniLM-L6-v2').\n",
    "- Takes a user's question and vectorizes it into an embedding using the pre-trained model.\n",
    "- Queries the ChromaDB collection to find the top 5 closest matches to the question's embedding.\n",
    "- Returns the most relevant document from the database to answer the user's question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model for vectorization\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Load the embedding model\n",
    "\n",
    "def retrieve_relevant_data(question, collection, model):\n",
    "    # Vectorize the input question\n",
    "    question_embedding = model.encode([question])[0]\n",
    "    # Retrieve the closest match from ChromaDB\n",
    "    results = collection.query(query_embeddings=[question_embedding], n_results=5)\n",
    "    return results['documents'][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up ChromaDB as a Retriever with Hugging Face Embeddings\n",
    "This code initializes ChromaDB as a vector store, using embeddings from a Hugging Face model to retrieve relevant data based on vector similarity.\n",
    "\n",
    "- Loads the HuggingFaceEmbeddings model (all-MiniLM-L6-v2) for embedding text into vectors.\n",
    "- Sets up ChromaDB as a vector store with the collection \"personal_data\" and associates it with the embeddings model for future queries.\n",
    "- Converts the ChromaDB collection into a retriever by enabling similarity-based search (k=5), allowing the retrieval of the top 5 most relevant documents based on query embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emdfa\\AppData\\Local\\Temp\\ipykernel_9588\\1812738755.py:5: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "C:\\Users\\emdfa\\AppData\\Local\\Temp\\ipykernel_9588\\1812738755.py:8: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vector_store = Chroma(\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Initialize embeddings model\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Initialize ChromaDB vector store with collection name and embeddings\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"personal_data\",\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "# Convert ChromaDB collection to a retriever\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing OpenAI's GPT-3.5-Turbo for Language Model Integration\n",
    "This code snippet initializes OpenAI's GPT-3.5-turbo model through the LangChain framework, enabling its use for generating responses in a Retrieval-Augmented Generation (RAG) system.\n",
    "\n",
    "- Imports ChatOpenAI from langchain.chat_models to interact with OpenAI's chat-based language models.\n",
    "- Initializes the GPT-3.5-turbo model by providing the OpenAI API key and model name (gpt-3.5-turbo), setting up the model for generating AI-driven responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emdfa\\AppData\\Local\\Temp\\ipykernel_9588\\3323242189.py:5: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(openai_api_key=\"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\", model=\"gpt-3.5-turbo\")\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Initialize OpenAI API\n",
    "llm = ChatOpenAI(openai_api_key=\"Your_OpenAI_api_key\", model=\"gpt-3.5-turbo\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up a Retrieval-Augmented Generation (RAG) Chain for Question Answering\n",
    "This code creates a RetrievalQA chain that combines a language model with a retriever, enabling the system to answer questions based on relevant documents from ChromaDB.\n",
    "\n",
    "- Initializes a RetrievalQA chain using the GPT-3.5-turbo model (llm) and the Chroma retriever (retriever) to process queries.\n",
    "- Enables the option to return the source documents used to generate the answer, providing transparency and context for the AI-generated response.\n",
    "- Asks a question (\"What is my experience in Data Science?\") using the chain, which retrieves relevant documents and generates a response.\n",
    "- Prints the generated answer, while optionally printing the retrieved documents for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emdfa\\AppData\\Local\\Temp\\ipykernel_9588\\1501611839.py:12: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa_chain({\"query\": question})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Answer: Based on the provided context, you have experience as a physicist, data scientist, and AI/ML specialist in both academic and corporate settings. You are skilled in data analysis, machine learning, and problem-solving, with a focus on applying AI/ML solutions for business insights and research. Your experience includes designing and delivering comprehensive courses in data analytics and data science as an online educator. You also have strong communication and analytical skills with a proven ability to work independently.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Set up the retrieval QA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True  # Optional: to return the documents retrieved from Chroma\n",
    ")\n",
    "\n",
    "# Ask a question using the chain\n",
    "question = \"What is my experience in Data Science?\"\n",
    "result = qa_chain({\"query\": question})\n",
    "\n",
    "# Print the generated answer and the source documents\n",
    "print(\"Generated Answer:\", result['result'])\n",
    "#print(\"Retrieved Documents:\", result['source_documents'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Conversational Retrieval Chain with Memory for Follow-up Questions\n",
    "This code sets up a Conversational Retrieval Chain using OpenAI's GPT-3.5-turbo, a ChromaDB retriever, and memory to handle follow-up questions while maintaining context.\n",
    "\n",
    "- Conversation Buffer Memory - Initializes ConversationBufferMemory to store and manage the conversation history, allowing the AI to remember previous interactions.\n",
    "- Conversational Retrieval Chain - Uses from_llm to create a chain that retrieves relevant data from ChromaDB and generates responses while considering the conversation history.\n",
    "- First Question - Queries the user's experience in web development, retrieving relevant information and generating a response based on the context.\n",
    "- Follow-up Question - Asks a second question (\"Can you explain my most recent project?\"), where the chain uses memory to maintain the conversation's context and provide a coherent, contextually appropriate answer.\n",
    "\n",
    "Both questions and responses are printed, showcasing the conversational capabilities of the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emdfa\\AppData\\Local\\Temp\\ipykernel_9588\\1492616057.py:7: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
      "C:\\Users\\emdfa\\AppData\\Local\\Temp\\ipykernel_9588\\1492616057.py:18: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response1 = conversational_chain.run(question1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is my experience in web development?\n",
      "A: I don't have enough information to determine your experience in web development based on the context provided.\n",
      "Q: Can you explain my most recent project?\n",
      "A: I don't have access to the specific details of the user's most recent project in web development based on the provided context.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "# Initialize memory to store conversation context\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# Use the from_llm method to create a conversational retrieval chain\n",
    "conversational_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,  # ChromaDB retriever\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "# Ask the first question\n",
    "question1 = \"What is my experience in web development?\"\n",
    "response1 = conversational_chain.run(question1)\n",
    "print(f\"Q: {question1}\\nA: {response1}\")\n",
    "\n",
    "# Follow-up question\n",
    "question2 = \"Can you explain my most recent project?\"\n",
    "response2 = conversational_chain.run(question2)\n",
    "print(f\"Q: {question2}\\nA: {response2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up a Gradio Interface for Conversational AI with RAG\n",
    "This code creates a Gradio interface that allows users to ask questions, retrieves relevant information using a conversational retrieval chain, and generates AI responses.\n",
    "\n",
    "- Answer Function - Defines answer_question to handle user input and use the conversational chain to retrieve relevant data and generate a response.\n",
    "- Gradio Interface Setup - Uses Gradio to create an interactive web interface where users can ask questions in a text box (inputs=\"text\") and see the AI-generated response in another text box (outputs=\"text\").\n",
    "- Customization - Provides a title (\"Conversational AI with RAG\") and a description that invites users to ask questions about the AI's experience.\n",
    "- Launch - Calls gr_interface.launch() to start the Gradio web interface, enabling users to interact with the AI system directly via a browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7864\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Define a function to process user input and return AI response\n",
    "def answer_question(question):\n",
    "    response = conversational_chain.run(question)\n",
    "    return response\n",
    "\n",
    "# Set up the Gradio interface\n",
    "gr_interface = gr.Interface(\n",
    "    fn=answer_question,  # The function that processes user input\n",
    "    inputs=\"text\",  # Input type is a simple text box\n",
    "    outputs=\"text\",  # Output is a text box showing the AI's response\n",
    "    title=\"Conversational AI with RAG\",\n",
    "    description=\"Ask me anything about my experience!\"\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "gr_interface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
